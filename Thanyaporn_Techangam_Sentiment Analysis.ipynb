{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8971488",
   "metadata": {
    "id": "c8971488"
   },
   "source": [
    "# **Sentiment Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb4461a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fb4461a1",
    "outputId": "66e1870b-c82e-4b52-c19e-8418788cd62b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-26 14:02:54.987812: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-26 14:02:56.094351: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting en-core-web-lg==3.5.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.5.0/en_core_web_lg-3.5.0-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m587.7/587.7 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.6.0,>=3.5.0 in /usr/local/lib/python3.9/dist-packages (from en-core-web-lg==3.5.0) (3.5.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.4.6)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.22.4)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.8)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.27.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.7)\n",
      "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (6.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.8)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.1.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.3.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.10.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.1.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (23.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.65.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.0.12)\n",
      "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (67.7.2)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /usr/local/lib/python3.9/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (4.5.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2022.12.7)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.9/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.9/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->spacy<3.6.0,>=3.5.0->en-core-web-lg==3.5.0) (2.1.2)\n",
      "Installing collected packages: en-core-web-lg\n",
      "Successfully installed en-core-web-lg-3.5.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be8d2c64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "be8d2c64",
    "outputId": "e126df83-4ffb-40a9-eb8b-e28dcd7b688a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from vaderSentiment) (2.27.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->vaderSentiment) (2.0.12)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87122451",
   "metadata": {
    "id": "87122451"
   },
   "outputs": [],
   "source": [
    "# import some relevant libraries first\n",
    "\n",
    "# for data handling and manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# for natural language processing (NLP) tasks\n",
    "import nltk\n",
    "# for advanced NLP tasks such as entity recognition and dependency parsing (not used in this assignment)\n",
    "import spacy \n",
    "# for tokenization of text into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "# for accessing stop words used in English language\n",
    "from nltk.corpus import stopwords\n",
    "# for lemmatization of words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# for splitting data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "# for import lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media. \n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "# for evaluate the performance of the model using accurac\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85eb22",
   "metadata": {
    "id": "9c85eb22"
   },
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032f0b18",
   "metadata": {
    "id": "032f0b18"
   },
   "source": [
    "## Dataset \n",
    "\n",
    "-  'IMDB Dataset.csv' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "IJc5sGlgFlu6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IJc5sGlgFlu6",
    "outputId": "5f5c82cb-44e4-426a-fecc-0d201cf1db74"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61cbf2e3",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "61cbf2e3",
    "outputId": "4518894b-83a8-44fa-819f-7eee71d409a3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-c8472d38-3325-4ec2-8081-0e424d816834\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>I thought this movie did a down right good job...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>Bad plot, bad dialogue, bad acting, idiotic di...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>I am a Catholic taught in parochial elementary...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>I'm going to have to disagree with the previou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>No one expects the Star Trek movies to be high...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8472d38-3325-4ec2-8081-0e424d816834')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "        \n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "      \n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-c8472d38-3325-4ec2-8081-0e424d816834 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-c8472d38-3325-4ec2-8081-0e424d816834');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n",
       "  "
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      One of the other reviewers has mentioned that ...  positive\n",
       "1      A wonderful little production. <br /><br />The...  positive\n",
       "2      I thought this was a wonderful way to spend ti...  positive\n",
       "3      Basically there's a family where a little boy ...  negative\n",
       "4      Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
       "...                                                  ...       ...\n",
       "49995  I thought this movie did a down right good job...  positive\n",
       "49996  Bad plot, bad dialogue, bad acting, idiotic di...  negative\n",
       "49997  I am a Catholic taught in parochial elementary...  negative\n",
       "49998  I'm going to have to disagree with the previou...  negative\n",
       "49999  No one expects the Star Trek movies to be high...  negative\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data and check column details in table \n",
    "data = pd.read_csv('/content/drive/MyDrive/IMDB Dataset.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ehcaqmg9LNZi",
   "metadata": {
    "id": "Ehcaqmg9LNZi"
   },
   "source": [
    "### Preprocess the data, including tokenizing the text, removing stop words, converting the text into lowercase, and lemmatization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc8ee12",
   "metadata": {
    "id": "dcc8ee12"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\", disable=['parser', 'tagger', 'ner'])\n",
    "def normalize(review, lowercase, remove_stopwords):\n",
    "    if lowercase:\n",
    "        review = review.lower()\n",
    "    doc = nlp(review)\n",
    "    lemmatized = list()\n",
    "    for token in doc:\n",
    "        if not remove_stopwords or (remove_stopwords and not token.is_stop):\n",
    "            lemmatized.append(token.lemma_)\n",
    "    return \" \".join(lemmatized)\n",
    "data['processed'] = data['review'].apply(normalize, lowercase=True, remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zrB6tRItLV94",
   "metadata": {
    "id": "zrB6tRItLV94"
   },
   "source": [
    "### Partition the movie reviews into the training and test sets with an 80-20 split. Please make sure that the target variable in the training set and test sets follow the same distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "LvhFn4ufGKY7",
   "metadata": {
    "id": "LvhFn4ufGKY7"
   },
   "outputs": [],
   "source": [
    "#Splitting the data into trainig and testing and specifying 'stratify=data['sentiment']' to make sure that the target variable in the training set and test sets follow the same distribution.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data['review'], data['sentiment'], test_size=0.2, random_state=21, stratify=data['sentiment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_FtFwYAILwmm",
   "metadata": {
    "id": "_FtFwYAILwmm"
   },
   "source": [
    "## Lexicon-based sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GrQyD7nwL90N",
   "metadata": {
    "id": "GrQyD7nwL90N"
   },
   "source": [
    "### Build a lexicon-based sentiment analysis with VADER Sentiment Analysis tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1B0r3tPLwPd",
   "metadata": {
    "id": "a1B0r3tPLwPd"
   },
   "outputs": [],
   "source": [
    "# Initialize the SentimentIntensityAnalyzer object\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Define a function to calculate the sentiment scores using VADER\n",
    "def vader_sentiment_scores(review):\n",
    "    sentiment_scores = analyzer.polarity_scores(review)\n",
    "    return sentiment_scores['compound']\n",
    "\n",
    "# Apply the function to the processed text data to get the sentiment scores\n",
    "X_train_scores = X_train.apply(vader_sentiment_scores)\n",
    "X_test_scores = X_test.apply(vader_sentiment_scores)\n",
    "\n",
    "# Threshold the sentiment scores to classify the reviews as positive or negative \n",
    "# (set threshold as 0 aince there is only positive or negative for sentiment in this dataset)\n",
    "Y_train_pred = (X_train_scores > 0).astype(int)\n",
    "Y_test_pred = (X_test_scores > 0).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4s6SPaQCNcNz",
   "metadata": {
    "id": "4s6SPaQCNcNz"
   },
   "source": [
    "### Evaluate the performance of my model using accuracy, precision, recall, and F1 score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "M9_Y71dBVvc-",
   "metadata": {
    "id": "M9_Y71dBVvc-"
   },
   "source": [
    "**NOTE: 0= Negative and 1= Positive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "YIXww2cChQHJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YIXww2cChQHJ",
    "outputId": "c8ab0ecd-7727-4482-b91d-d24e3e3048b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.53      0.64      5000\n",
      "           1       0.65      0.87      0.74      5000\n",
      "\n",
      "    accuracy                           0.70     10000\n",
      "   macro avg       0.72      0.70      0.69     10000\n",
      "weighted avg       0.72      0.70      0.69     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Initialize the label encoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit the label encoder on the training labels\n",
    "label_encoder.fit(Y_train)\n",
    "\n",
    "# Convert the string labels to integers\n",
    "Y_train_encoded = label_encoder.transform(Y_train)\n",
    "Y_test_encoded = label_encoder.transform(Y_test)\n",
    "\n",
    "# Evaluate the performance of your model using accuracy, precision, recall, and F1 score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print('Test classification report:')\n",
    "print(classification_report(Y_test_encoded, Y_test_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4yB2TY_a1A_9",
   "metadata": {
    "id": "4yB2TY_a1A_9"
   },
   "source": [
    "### Naive Bayes model for sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "tjrH1JXkaxhZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tjrH1JXkaxhZ",
    "outputId": "1dcac6e3-0a89-4063-823c-3f381a760a0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'vectorizer': CountVectorizer(max_features=1500), 'model': MultinomialNB()}\n",
      "Accuracy: 0.8239\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.82      0.82      0.82      5000\n",
      "    positive       0.82      0.82      0.82      5000\n",
      "\n",
      "    accuracy                           0.82     10000\n",
      "   macro avg       0.82      0.82      0.82     10000\n",
      "weighted avg       0.82      0.82      0.82     10000\n",
      "\n",
      "Configuration: {'vectorizer': CountVectorizer(max_features=2500), 'model': MultinomialNB(alpha=0.5)}\n",
      "Accuracy: 0.831\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.83      0.83      5000\n",
      "    positive       0.83      0.83      0.83      5000\n",
      "\n",
      "    accuracy                           0.83     10000\n",
      "   macro avg       0.83      0.83      0.83     10000\n",
      "weighted avg       0.83      0.83      0.83     10000\n",
      "\n",
      "Configuration: {'vectorizer': CountVectorizer(max_features=5000), 'model': MultinomialNB(alpha=0.1)}\n",
      "Accuracy: 0.8397\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.85      0.84      5000\n",
      "    positive       0.85      0.83      0.84      5000\n",
      "\n",
      "    accuracy                           0.84     10000\n",
      "   macro avg       0.84      0.84      0.84     10000\n",
      "weighted avg       0.84      0.84      0.84     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Pre-Prcoessing and Bag of Word Vectorization using Count Vectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'vectorizer': CountVectorizer(max_features=1500),\n",
    "        'model': MultinomialNB(alpha=1.0)\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': CountVectorizer(max_features=2500),\n",
    "        'model': MultinomialNB(alpha=0.5)\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': CountVectorizer(max_features=5000),\n",
    "        'model': MultinomialNB(alpha=0.1)\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    # Vectorize the text data\n",
    "    vectorizer = config['vectorizer']\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a Naive Bayes model\n",
    "    nb = config['model']\n",
    "    nb.fit(X_train_vectorized, Y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    Y_pred = nb.predict(X_test_vectorized)\n",
    "    v_performance = metrics.classification_report(Y_test, Y_pred)\n",
    "    accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({'config': config, 'accuracy': accuracy, 'v_performance': v_performance})\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Configuration: {result['config']}\")\n",
    "    print(f\"Accuracy: {result['accuracy']}\")\n",
    "    print(f\"Classification Report:\\n {result['v_performance']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Rrvr7B8l01kZ",
   "metadata": {
    "id": "Rrvr7B8l01kZ"
   },
   "source": [
    "### Document of the configurations I have tested and performance report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324Yghrc09Z-",
   "metadata": {
    "id": "324Yghrc09Z-"
   },
   "source": [
    "After conducting experiments on three different configurations of **Naive Bayes models using CountVectorizer method** for performing sentiment analysis on a movie reviews dataset. The first configuration involved a maximum of 1500 features, which resulted in an accuracy of **0.8239**. The precision, recall, and f1-scores for the negative class were 0.82 for all performance metrics, and for the positive class, they were 0.82 for all performance metrics as well. In the second configuration, I increased the number of features to 2500 and set the alpha value of MultinomialNB to 0.5, which led to an accuracy of **0.831** with similar precision, recall, and f1-scores as the first configuration(the negative class were 0.83 for all performance metrics, and for the positive class, they were 0.83 for all performance metrics as well). The third configuration involved a maximum of 5000 features and an alpha value of 0.1, which gave me the higehest accuracy of **0.8397** compared to others, with similar precision, recall, and f1-scores as the previous configurations(negative 0.83, 0.85 and 0.84/ positive  0.85, 0.83, 0.84). **Therefore, the last or third configuration is the one with best performance of F1-score and Accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GP9oAxTu0-FT",
   "metadata": {
    "id": "GP9oAxTu0-FT"
   },
   "source": [
    "## SVM model for sentiment analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6iFRgF91jDos",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6iFRgF91jDos",
    "outputId": "80273374-70a8-49a6-b52d-438c181bdea5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration: {'vectorizer': TfidfVectorizer(max_features=1500), 'model': LinearSVC()}\n",
      "Accuracy: 0.8793\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.89      0.87      0.88      5000\n",
      "    positive       0.87      0.89      0.88      5000\n",
      "\n",
      "    accuracy                           0.88     10000\n",
      "   macro avg       0.88      0.88      0.88     10000\n",
      "weighted avg       0.88      0.88      0.88     10000\n",
      "\n",
      "Configuration: {'vectorizer': TfidfVectorizer(max_features=2500), 'model': LinearSVC(C=0.5)}\n",
      "Accuracy: 0.8909\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.90      0.87      0.89      5000\n",
      "    positive       0.88      0.91      0.89      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n",
      "Configuration: {'vectorizer': TfidfVectorizer(max_features=5000), 'model': LinearSVC(C=0.1)}\n",
      "Accuracy: 0.8959\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.91      0.88      0.89      5000\n",
      "    positive       0.88      0.91      0.90      5000\n",
      "\n",
      "    accuracy                           0.90     10000\n",
      "   macro avg       0.90      0.90      0.90     10000\n",
      "weighted avg       0.90      0.90      0.90     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Pre-processing and Bag of Word Vectorization using TF-IDF Vectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics\n",
    "\n",
    "configs = [\n",
    "    {\n",
    "        'vectorizer': TfidfVectorizer(max_features=1500),\n",
    "        'model': LinearSVC(C=1.0)\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': TfidfVectorizer(max_features=2500),\n",
    "        'model': LinearSVC(C=0.5)\n",
    "    },\n",
    "    {\n",
    "        'vectorizer': TfidfVectorizer(max_features=5000),\n",
    "        'model': LinearSVC(C=0.1)\n",
    "    }\n",
    "]\n",
    "\n",
    "results = []\n",
    "for config in configs:\n",
    "    # Vectorize the text data\n",
    "    vectorizer = config['vectorizer']\n",
    "    X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "    X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "    # Train a LinearSVC model\n",
    "    svm = config['model']\n",
    "    svm.fit(X_train_vectorized, Y_train)\n",
    "\n",
    "    # Make predictions and evaluate the model\n",
    "    Y_pred = svm.predict(X_test_vectorized)\n",
    "    v_performance = metrics.classification_report(Y_test, Y_pred)\n",
    "    accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "\n",
    "    # Store the results\n",
    "    results.append({'config': config, 'accuracy': accuracy, 'v_performance': v_performance})\n",
    "\n",
    "# Print the results\n",
    "for result in results:\n",
    "    print(f\"Configuration: {result['config']}\")\n",
    "    print(f\"Accuracy: {result['accuracy']}\")\n",
    "    print(f\"Classification Report:\\n {result['v_performance']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uNNbLiWE6Fcf",
   "metadata": {
    "id": "uNNbLiWE6Fcf"
   },
   "source": [
    "### Document of the configurations I have tested and performance report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "itAo7TU26Qqs",
   "metadata": {
    "id": "itAo7TU26Qqs"
   },
   "source": [
    "After conducting experiments on three different configurations of **support vector machine (SVC) models using TfidfVectorizer method**  for performing sentiment analysis on a movie reviews dataset. The first configuration involved a maximum of 1500 features, which resulted in an accuracy of **0.8793**. The precision, recall, and f1-scores for the negative class were 0.89, 0.87 and 0.88, respectively, and for the positive class, they were 0.87, 0.89 and 0.88, respectively. In the second configuration, I increased the number of features to 2500 and set the alpha value of LinearSVC to 0.5, which led to an accuracy of **0.8909** with similar precision, recall, and f1-scores as the first configuration (for negative 0.90, 0.87 and 0.89/ positive 0.88, 0.91 and 0.89) . The third configuration involved a maximum of 5000 features and a LinearSVC value of 0.1, which gave us the highest accuracy of **0.8959** compared to others, with similar precision, recall, and f1-scores as the previous configurations(for negative 0.91, 0.88 and 0.89/ positive 0.88, 0.91 and 0.90). **Therefore, the last or third configuration with the number of features to 5000 and set the alpha value of LinearSVC to 0.1 is the one with best performance of F1-score and Accuracy.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cFNt-xJm6W1r",
   "metadata": {
    "id": "cFNt-xJm6W1r"
   },
   "source": [
    "## Deep Learning Models for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "MkkG-cxZ6W8F",
   "metadata": {
    "id": "MkkG-cxZ6W8F"
   },
   "source": [
    "### Preprocess the movie reviews into sequences of equal length for deep learning models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "xCFwVmiP9BS5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xCFwVmiP9BS5",
    "outputId": "f0d97fbc-188c-416d-c37c-f52538479575"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: keras in /usr/local/lib/python3.9/dist-packages (2.12.0)\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "E1h0arLT9JEw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1h0arLT9JEw",
    "outputId": "4e440efd-c9ad-4b5c-bad7-82b05d916fef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib/python3.9/dist-packages (2.12.0)\n",
      "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.3.3)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.32.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.2)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.12.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (2.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from tensorflow) (23.1)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.8.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from tensorflow) (67.7.2)\n",
      "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (0.4.8)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.53.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (3.20.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (4.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.16.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (16.0.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.9/dist-packages (from tensorflow) (1.4.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.9/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
      "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.9/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.9/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.9/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.9/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.9/dist-packages (from markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (6.4.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.9/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.9/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.13,>=2.12->tensorflow) (3.15.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.9/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.9/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "MZ7T9JBz61e0",
   "metadata": {
    "id": "MZ7T9JBz61e0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Create a tokenizer and fit it on the training data\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert the text data to sequence data\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad the sequence data to make them all the same length\n",
    "max_length = 100\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jj2b-Qr66fmt",
   "metadata": {
    "id": "Jj2b-Qr66fmt"
   },
   "source": [
    "### Train a Bidirectional LSTM model for sentiment classification, set early stopping conditions, and use a validation set. Evaluate the model performance on the test set using accuracy, precision, recall, and F1-score. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gNvqf2iFpva7",
   "metadata": {
    "id": "gNvqf2iFpva7"
   },
   "source": [
    "### **Configuration 5.1: Locally trained embeddings**\n",
    "A text classification model using Bidirectional LSTMs with a single layer and an embedding dimension of 100. The model is trained using a tokenizer with a maximum vocabulary size of 10000 and padded sequences with a maximum length of 100. The training process runs for a maximum of 15 epochs and includes early stopping with a patience of 3 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "N9XfXk0v1ZMJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9XfXk0v1ZMJ",
    "outputId": "dd11a48a-29fc-40ed-b186-f8eb3890bf90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "282/282 [==============================] - 140s 480ms/step - loss: 0.3955 - accuracy: 0.8156 - val_loss: 0.3154 - val_accuracy: 0.8720\n",
      "Epoch 2/15\n",
      "282/282 [==============================] - 136s 481ms/step - loss: 0.2539 - accuracy: 0.8973 - val_loss: 0.3214 - val_accuracy: 0.8700\n",
      "Epoch 3/15\n",
      "282/282 [==============================] - 135s 478ms/step - loss: 0.1951 - accuracy: 0.9235 - val_loss: 0.3701 - val_accuracy: 0.8683\n",
      "Epoch 4/15\n",
      "282/282 [==============================] - 137s 486ms/step - loss: 0.1422 - accuracy: 0.9467 - val_loss: 0.4068 - val_accuracy: 0.8525\n",
      "313/313 [==============================] - 13s 39ms/step\n",
      "Accuracy: 0.8511\n",
      "Precision: 0.8260\n",
      "Recall: 0.8896\n",
      "F1-score: 0.8566\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Preprocess the data\n",
    "max_words = 10000\n",
    "max_len = 100\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_len)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_len)\n",
    "\n",
    "# Encode target variable\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = to_categorical(encoder.fit_transform(Y_train))\n",
    "y_test_encoded = to_categorical(encoder.transform(Y_test))\n",
    "\n",
    "# Create the Bidirectional LSTM model with locally trained embeddings\n",
    "model = Sequential([\n",
    "    Embedding(max_words, 100, input_length=max_len),\n",
    "    Bidirectional(LSTM(64)),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set early stopping condition\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train_encoded, batch_size=128, epochs=15, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "y_pred_proba = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(encoder.transform(Y_test), y_pred)\n",
    "precision = precision_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "recall = recall_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "f1 = f1_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nRDgDRpwp_n5",
   "metadata": {
    "id": "nRDgDRpwp_n5"
   },
   "source": [
    "### **Configuration 5.2: Pre-trained embeddings with a single layer of Bidirectional LSTMs**\n",
    "\n",
    "A text classification model using Bidirectional LSTMs with a single layer and an embedding dimension of 100. The model is trained using a tokenizer with a maximum vocabulary size of 10000 and padded sequences with a maximum length of 200. The training process runs for a maximum of 10 epochs and includes early stopping with a patience of 3 to prevent overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "NvRITU9zhdsF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NvRITU9zhdsF",
    "outputId": "67a436a2-3e60-46ff-8a83-972bfd64f7b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Epoch 1/10\n",
      "563/563 [==============================] - 325s 568ms/step - loss: 0.6446 - accuracy: 0.6271 - val_loss: 0.5863 - val_accuracy: 0.7195\n",
      "Epoch 2/10\n",
      "563/563 [==============================] - 259s 460ms/step - loss: 0.5182 - accuracy: 0.7395 - val_loss: 0.3931 - val_accuracy: 0.8278\n",
      "Epoch 3/10\n",
      "563/563 [==============================] - 271s 482ms/step - loss: 0.3780 - accuracy: 0.8347 - val_loss: 0.3549 - val_accuracy: 0.8472\n",
      "Epoch 4/10\n",
      "563/563 [==============================] - 264s 468ms/step - loss: 0.3445 - accuracy: 0.8500 - val_loss: 0.3368 - val_accuracy: 0.8550\n",
      "Epoch 5/10\n",
      "563/563 [==============================] - 258s 459ms/step - loss: 0.3214 - accuracy: 0.8632 - val_loss: 0.3341 - val_accuracy: 0.8533\n",
      "Epoch 6/10\n",
      "563/563 [==============================] - 279s 496ms/step - loss: 0.3037 - accuracy: 0.8719 - val_loss: 0.3298 - val_accuracy: 0.8545\n",
      "Epoch 7/10\n",
      "563/563 [==============================] - 261s 463ms/step - loss: 0.2865 - accuracy: 0.8785 - val_loss: 0.3145 - val_accuracy: 0.8622\n",
      "Epoch 8/10\n",
      "563/563 [==============================] - 264s 469ms/step - loss: 0.2705 - accuracy: 0.8861 - val_loss: 0.3085 - val_accuracy: 0.8702\n",
      "Epoch 9/10\n",
      "563/563 [==============================] - 257s 457ms/step - loss: 0.2536 - accuracy: 0.8961 - val_loss: 0.3070 - val_accuracy: 0.8680\n",
      "Epoch 10/10\n",
      "563/563 [==============================] - 262s 465ms/step - loss: 0.2417 - accuracy: 0.9004 - val_loss: 0.3066 - val_accuracy: 0.8723\n",
      "313/313 [==============================] - 25s 78ms/step\n",
      "Accuracy: 0.8853\n",
      "Precision: 0.8966\n",
      "Recall: 0.8710\n",
      "F1-score: 0.8836\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive to access files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the GloVe embeddings file and directory paths\n",
    "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "glove_dir = '/content/drive/My Drive/embeddings'\n",
    "glove_file = os.path.join(glove_dir, 'glove.6B.100d.txt')\n",
    "zip_file = os.path.join(glove_dir, 'glove.6B.zip')\n",
    "\n",
    "# Download and extract the GloVe embeddings file if it doesn't exist\n",
    "if not os.path.exists(glove_dir):\n",
    "    os.makedirs(glove_dir)\n",
    "urllib.request.urlretrieve(glove_url, zip_file)\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(glove_dir)\n",
    "\n",
    "# Load the embeddings into memory\n",
    "embeddings_index = {}\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Tokenize and pad sequences\n",
    "tokenizer = Tokenizer(num_words=10000)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "max_length = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length, padding='post')\n",
    "\n",
    "# Encode target variable\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = to_categorical(encoder.fit_transform(Y_train))\n",
    "y_test_encoded = to_categorical(encoder.transform(Y_test))\n",
    "\n",
    "# Create the embedding matrix\n",
    "embedding_dim = 100\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Define the Bidirectional LSTM model with GloVe embeddings\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set early stopping condition and train the model\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(X_train_pad, y_train_encoded, batch_size=64, epochs=10, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "y_pred_proba = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(encoder.transform(Y_test), y_pred)\n",
    "precision = precision_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "recall = recall_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "f1 = f1_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "\n",
    "# Print the evaluation metrics on the test dataset\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OzO2qeMHqGhU",
   "metadata": {
    "id": "OzO2qeMHqGhU"
   },
   "source": [
    "### **Configuration 5.3: Pre-trained embeddings with 2 layers of Bidirectional LSTMs**\n",
    "\n",
    "A text classification model using Bidirectional LSTMs with 2 layers and an embedding dimension of 100. The model is trained using a tokenizer with a maximum vocabulary size of 10000 and padded sequences with a maximum length of 200. The training process runs for a maximum of 8 epochs and includes early stopping with a patience of 3 to prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "VDOn5ZG72Ekl",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDOn5ZG72Ekl",
    "outputId": "f9f68e16-5212-400e-f06a-3a4337673972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "Epoch 1/8\n",
      "282/282 [==============================] - 2023s 7s/step - loss: 0.5441 - accuracy: 0.7197 - val_loss: 0.4448 - val_accuracy: 0.8005\n",
      "Epoch 2/8\n",
      "282/282 [==============================] - 1972s 7s/step - loss: 0.4322 - accuracy: 0.8032 - val_loss: 0.4037 - val_accuracy: 0.8195\n",
      "Epoch 3/8\n",
      "282/282 [==============================] - 1944s 7s/step - loss: 0.3912 - accuracy: 0.8256 - val_loss: 0.3543 - val_accuracy: 0.8428\n",
      "Epoch 4/8\n",
      "282/282 [==============================] - 1966s 7s/step - loss: 0.3418 - accuracy: 0.8522 - val_loss: 0.3148 - val_accuracy: 0.8610\n",
      "Epoch 5/8\n",
      "282/282 [==============================] - 1955s 7s/step - loss: 0.3202 - accuracy: 0.8632 - val_loss: 0.3134 - val_accuracy: 0.8618\n",
      "Epoch 6/8\n",
      "282/282 [==============================] - 1939s 7s/step - loss: 0.2988 - accuracy: 0.8738 - val_loss: 0.2894 - val_accuracy: 0.8725\n",
      "Epoch 7/8\n",
      "282/282 [==============================] - 1936s 7s/step - loss: 0.2893 - accuracy: 0.8781 - val_loss: 0.2827 - val_accuracy: 0.8795\n",
      "Epoch 8/8\n",
      "282/282 [==============================] - 1938s 7s/step - loss: 0.2738 - accuracy: 0.8843 - val_loss: 0.2832 - val_accuracy: 0.8792\n",
      "313/313 [==============================] - 98s 308ms/step\n",
      "Accuracy: 0.8864\n",
      "Precision: 0.9149\n",
      "Recall: 0.8520\n",
      "F1-score: 0.8824\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import urllib\n",
    "import zipfile\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive to access files\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Define the GloVe embeddings file and directory paths\n",
    "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
    "glove_dir = '/content/drive/My Drive/embeddings'\n",
    "glove_file = os.path.join(glove_dir, 'glove.6B.100d.txt')\n",
    "zip_file = os.path.join(glove_dir, 'glove.6B.zip')\n",
    "\n",
    "# Download and extract the GloVe embeddings file if it doesn't exist\n",
    "if not os.path.exists(glove_dir):\n",
    "    os.makedirs(glove_dir)\n",
    "urllib.request.urlretrieve(glove_url, zip_file)\n",
    "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
    "    zip_ref.extractall(glove_dir)\n",
    "\n",
    "# Load the embeddings into memory\n",
    "embeddings_index = {}\n",
    "with open(glove_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "# Create tokenizer and sequence padding\n",
    "max_words = 10000\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "max_length = 200\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_length)\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_length)\n",
    "\n",
    "# Create embedding matrix\n",
    "embedding_dim = 100\n",
    "embedding_matrix = np.zeros((max_words, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= max_words:\n",
    "        break\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "# Encode target variable\n",
    "encoder = LabelEncoder()\n",
    "y_train_encoded = to_categorical(encoder.fit_transform(Y_train))\n",
    "y_test_encoded = to_categorical(encoder.transform(Y_test))\n",
    "\n",
    "# Create and compile model\n",
    "model = Sequential([\n",
    "    Embedding(max_words, embedding_dim, input_length=max_length, weights=[embedding_matrix], trainable=False),\n",
    "    Bidirectional(LSTM(128, dropout=0.2, recurrent_dropout=0.2, return_sequences=True)),\n",
    "    Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2)),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Set early stopping condition\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_pad, y_train_encoded, batch_size=128, epochs=8, validation_split=0.1, callbacks=[early_stopping])\n",
    "\n",
    "# Evaluate the model performance\n",
    "y_pred_proba = model.predict(X_test_pad)\n",
    "y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "\n",
    "accuracy = accuracy_score(encoder.transform(Y_test), y_pred)\n",
    "precision = precision_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "recall = recall_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "f1 = f1_score(Y_test, encoder.inverse_transform(y_pred), pos_label='positive', average='binary')\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cGpVHSaj6itX",
   "metadata": {
    "id": "cGpVHSaj6itX"
   },
   "source": [
    "### Document of the configurations I have tested and performance report."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hgeBCbn-VkBU",
   "metadata": {
    "id": "hgeBCbn-VkBU"
   },
   "source": [
    "For sentiment analysis, I have tested three different configurations of Bidirectional LSTM models using the preprocessed movie review dataset. The first configuration used locally trained embeddings, the second configuration used pre-trained embeddings with a single layer of Bidirectional LSTMs, and the third configuration used pre-trained embeddings with 2 layers of Bidirectional LSTMs. All models were trained with early stopping and a validation set to prevent overfitting. The performance of the models was evaluated on the test set using accuracy, precision, recall, and F1-score. Accuracy, Precision, Recall and F1-score for the first configuration are 0.8511, 0.8260, 0.8896 and 0.8566, respectively. while for the second configuration, Accuracy, Precision, Recall and F1-score are 0.8853, 0.8966, 0.8710 and 0.8836. However, the third configuration achieved the best performance with **an accuracy of 0.8864, precision of 0.9149, recall of 0.8520, and an F1-score of 0.8824. Therefore, the third configuration of Pre-trained embeddings with 2 layers of Bidirectional LSTMs is the one selected for sentiment analysis in this scenario.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jSSep_ry6k3X",
   "metadata": {
    "id": "jSSep_ry6k3X"
   },
   "source": [
    "## Take away message"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nm2l_I3o6qCb",
   "metadata": {
    "id": "nm2l_I3o6qCb"
   },
   "source": [
    "### My thoughts and findings of the different models I have tried for sentiment classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oZKeY17H6rsX",
   "metadata": {
    "id": "oZKeY17H6rsX"
   },
   "source": [
    "From the different models that I have tried for sentiment classification, my thoughts and findings are that, compared to lexicon-based, Naive Bayes, SVM, and deep learning, the approach with the highest performance in my script is  SVM followed by deep learning, Naive Bayes, and lexicon-based respectively. In Task 3 and 4, I also noticed that using Tf-idf vectorizer is better than Count vectorizer as its accuracy, precision, recall, and F1 score are much higher. However, in each approach, the performance will depend on our judgment in deciding which parameter we will be tweaking. (But I have found that increasing the number of max features improves performance.)\n",
    "\n",
    "For Task 5, apart from discovering that it takes a long time to run an epoch, my thoughts and findings are that to use pre-trained embeddings will give better performance than locally trained embeddings because they are trained on larger and more varied collections of text, making them more robust and generalizable to a wide range of NLP tasks. Furthermore, the number of epochs might not be related to high performance. This is because using early stopping with a patience of 3 (which means it will stop training if the validation loss does not improve for 3 epochs), the first configuration, which is locally trained embeddings, although trained for 15 epochs more than the third configuration with only 8 epochs, shows less accuracy than both the second and third configuration which has only 10 and 8 epochs, respectively. Therefore, the number of epochs may not be related, while Bidirectional LSTMs with more layers will affect the performance of the model more. \n",
    "\n",
    "**Note:** Since it took a long time to run (7+ hours), there is still room for further development to achieve better performance than what was achieved with these models. Therefore, my thoughts and findings are limited to these models only. For task 5, using pre-trained embeddings with 2 layers of bidirectional LSTMs (Configuration 3) can achieve even higher performance than the support vector machine (SVM) models using the TfidfVectorizer method in task 4 if trained for more than 8 epochs as in this case, I also set early stopping with a patience of 3, but the model continued to run and gave higher performance in each epoch. Additionally, using larger pre-trained language models may also lead to improved performance. \n",
    "  \n",
    " "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
